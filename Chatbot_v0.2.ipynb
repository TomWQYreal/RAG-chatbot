{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA device\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "Error: 'StringPromptValue' object has no attribute 'replace'\n",
      "\n",
      "Query 1:\n",
      "Question: What are the salary and review for Dr. Smith?\n",
      "Answer: I'm sorry, I encountered an error while processing your request.\n",
      "\n",
      "Query 2:\n",
      "Question: What is the current wait time at Mercy Hospital?\n",
      "Answer: I'm sorry, I encountered an error while processing your request.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA, LLMChain\n",
    "from langchain.agents import Tool, initialize_agent, AgentType\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "import torch\n",
    "from langchain import PromptTemplate\n",
    "# Step 1: Load Data\n",
    "hospitals_df = pd.read_csv('hospitals.csv')\n",
    "patients_df = pd.read_csv('patients.csv')\n",
    "payers_df = pd.read_csv('payers.csv')\n",
    "physicians_df = pd.read_csv('physicians.csv')\n",
    "reviews_df = pd.read_csv('reviews.csv')\n",
    "visits_df = pd.read_csv('visits.csv')\n",
    "\n",
    "# Handle missing values function\n",
    "def handle_missing_values(df, strategy='fill'):\n",
    "    if strategy == 'fill':\n",
    "        return df.fillna('')\n",
    "    elif strategy == 'drop':\n",
    "        return df.dropna()\n",
    "    else:\n",
    "        raise ValueError(\"Strategy not recognized. Use 'fill' or 'drop'.\")\n",
    "\n",
    "# Prepare data (your code adapted)\n",
    "# 1. Prepare texts for each dataset separately\n",
    "dataset_texts = {\n",
    "    'hospitals': [],\n",
    "    'payers': [],\n",
    "    'physicians': [],\n",
    "    'patient_reviews': [],\n",
    "    'visits': [],\n",
    "    'physician_reviews': [],\n",
    "    'patient_data': []\n",
    "}\n",
    "\n",
    "# Hospitals\n",
    "hospitals_df['combined_text'] = (\n",
    "    \"Hospital Name: \" + hospitals_df['hospital_name'] +\n",
    "    \", State: \" + hospitals_df['hospital_state']\n",
    ")\n",
    "dataset_texts['hospitals'].extend(hospitals_df['combined_text'].dropna().tolist())\n",
    "\n",
    "# Payers\n",
    "payers_df['combined_text'] = \"Payer Name: \" + payers_df['payer_name']\n",
    "dataset_texts['payers'].extend(payers_df['combined_text'].dropna().tolist())\n",
    "\n",
    "# Physicians\n",
    "physicians_df['combined_text'] = (\n",
    "    \"Physician Name: Dr. \" + physicians_df['physician_name'] +\n",
    "    \", Medical School: \" + physicians_df['medical_school'] +\n",
    "    \", Graduation Year: \" + physicians_df['physician_grad_year'].astype(str) +\n",
    "    \", Salary: $\" + physicians_df['salary'].astype(str)\n",
    ")\n",
    "dataset_texts['physicians'].extend(physicians_df['combined_text'].dropna().tolist())\n",
    "\n",
    "# Patient Reviews\n",
    "dataset_texts['patient_reviews'].extend(reviews_df['review'].dropna().tolist())\n",
    "\n",
    "# Visits\n",
    "visits_df['combined_text'] = (\n",
    "    \"Chief Complaint: \" + visits_df['chief_complaint'] +\n",
    "    \", Treatment Description: \" + visits_df['treatment_description'] +\n",
    "    \", Primary Diagnosis: \" + visits_df['primary_diagnosis'] +\n",
    "    \", Billing Amount: $\" + visits_df['billing_amount'].astype(str)\n",
    ")\n",
    "dataset_texts['visits'].extend(visits_df['combined_text'].dropna().tolist())\n",
    "\n",
    "# Physician Reviews\n",
    "physician_reviews_df = reviews_df.groupby('physician_name')['review'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "for idx, row in physician_reviews_df.iterrows():\n",
    "    physician_name = row['physician_name']\n",
    "    reviews_text = row['review']\n",
    "    combined_text = f\"Physician Name: Dr. {physician_name}, Reviews: {reviews_text}\"\n",
    "    dataset_texts['physician_reviews'].append(combined_text)\n",
    "\n",
    "# Patient Data\n",
    "# Prepare patient data as per your code\n",
    "patient_reviews = reviews_df.merge(patients_df[['patient_id', 'patient_name']], on='patient_name', how='left')\n",
    "patient_reviews_grouped = patient_reviews.groupby(['patient_id', 'patient_name'])['review'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "patient_costs = visits_df.groupby('patient_id')['billing_amount'].agg(['sum', 'mean']).reset_index()\n",
    "patient_costs.rename(columns={'sum': 'total_cost', 'mean': 'average_cost'}, inplace=True)\n",
    "patient_costs = patient_costs.merge(patients_df[['patient_id', 'patient_name']], on='patient_id', how='left')\n",
    "patient_visits_info = visits_df.groupby('patient_id').agg({\n",
    "    'chief_complaint': lambda x: ' | '.join(x.dropna().unique()),\n",
    "    'treatment_description': lambda x: ' | '.join(x.dropna().unique())\n",
    "}).reset_index()\n",
    "patient_visits_info = patient_visits_info.merge(patients_df[['patient_id', 'patient_name']], on='patient_id', how='left')\n",
    "patient_data = patient_reviews_grouped.merge(patient_costs, on=['patient_id', 'patient_name'], how='outer')\n",
    "patient_data = patient_data.merge(patient_visits_info, on=['patient_id', 'patient_name'], how='outer')\n",
    "patient_data = handle_missing_values(patient_data, strategy='fill')\n",
    "patient_data['chief_complaint'] = patient_data['chief_complaint'].replace(\"\", \"No record\")\n",
    "patient_data['treatment_description'] = patient_data['treatment_description'].replace(\"\", \"No record\")\n",
    "# Initialize a list to hold patient texts\n",
    "patient_texts = []\n",
    "for idx, row in patient_data.iterrows():\n",
    "    patient_name = row['patient_name']\n",
    "    reviews_text = row['review']\n",
    "    total_cost = row['total_cost']\n",
    "    average_cost = row['average_cost']\n",
    "    chief_complaints = row['chief_complaint']\n",
    "    treatments = row['treatment_description']\n",
    "    # Create a combined text for the patient\n",
    "    combined_text = f\"Patient Name: {patient_name}, \"\n",
    "    combined_text += f\"Total Cost: {total_cost}, Average Cost per Visit: {average_cost}, \"\n",
    "    combined_text += f\"Chief Complaints: {chief_complaints}, \"\n",
    "    combined_text += f\"Treatments Received: {treatments}, \"\n",
    "    combined_text += f\"Reviews: {reviews_text}\"\n",
    "    patient_texts.append(combined_text)\n",
    "dataset_texts['patient_data'].extend(patient_texts)\n",
    "\n",
    "\n",
    "# Step 2: Chunk Texts with Metadata\n",
    "def chunk_text_with_metadata(texts, dataset_name, max_length=500, overlap=50):\n",
    "    chunks = []\n",
    "    for text in texts:\n",
    "        words = text.split()\n",
    "        start = 0\n",
    "        while start < len(words):\n",
    "            end = min(start + max_length, len(words))\n",
    "            chunk = ' '.join(words[start:end])\n",
    "            chunks.append({\n",
    "                'text': chunk,\n",
    "                'dataset': dataset_name\n",
    "            })\n",
    "            start += max_length - overlap\n",
    "    return chunks\n",
    "\n",
    "all_chunks = []\n",
    "for dataset_name, texts in dataset_texts.items():\n",
    "    # Adjust chunking parameters as needed\n",
    "    dataset_chunks = chunk_text_with_metadata(texts, dataset_name, max_length=500, overlap=100)\n",
    "    all_chunks.extend(dataset_chunks)\n",
    "\n",
    "chunked_texts = [chunk['text'] for chunk in all_chunks]\n",
    "chunked_metadatas = [{'dataset': chunk['dataset']} for chunk in all_chunks]\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Create the Chroma Vector Store with Embeddings\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vector_store = Chroma.from_texts(\n",
    "    texts=chunked_texts,\n",
    "    embedding=embedding_model,\n",
    "    metadatas=chunked_metadatas,\n",
    "    persist_directory=\"chroma_db\"\n",
    ")\n",
    "\n",
    "vector_store.persist()\n",
    "\n",
    "# Initialize the retriever\n",
    "retriever = vector_store.as_retriever()\n",
    "def retrieve_top_k(query, k=3):\n",
    "    docs = retriever.get_relevant_documents(query)[:k]\n",
    "    return docs\n",
    "\n",
    "# Step 4: Initialize the Local LLaMA Model with CUDA Support\n",
    "\n",
    "# Load the tokenizer and model\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "# Replace 'YourModelName' with the actual model you want to use\n",
    "# For example, 'decapoda-research/llama-7b-hf' (if you have access)\n",
    "tokenizer = LlamaTokenizer.from_pretrained('NousResearch/Llama-2-7b-hf')\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    'NousResearch/Llama-2-7b-hf',\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=None\n",
    ")\n",
    "\n",
    "# Move the model to CUDA if available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    model.to(device)\n",
    "    print(\"Using CUDA device\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using CPU device\")\n",
    "\n",
    "# Define the text generation function\n",
    "def generate_text(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=True,\n",
    "        temperature=0.5,\n",
    "        top_p=0.95,\n",
    "        repetition_penalty=1.15\n",
    "    )\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Create a custom LLM class\n",
    "from langchain.llms.base import LLM\n",
    "from typing import Optional, List\n",
    "\n",
    "class CustomLLM(LLM):\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        return generate_text(prompt)\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom\"\n",
    "\n",
    "# Initialize the custom LLM\n",
    "llm = CustomLLM()\n",
    "\n",
    "# Step 5: Create a Custom Prompt Template\n",
    "template = \"\"\"\n",
    "You are an intelligent assistant that provides detailed and accurate answers based on the provided context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Step 6: Create a Custom LLM Chain with the Custom Prompt\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Step 7: Create the RetrievalQA Chain with the Custom LLM Chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm_chain,\n",
    "    chain_type=\"stuff\",  # You can explore other chain types like \"map_reduce\" if needed\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    verbose=True\n",
    "    max_iterations=3,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "# Step 6: Define Custom Functions (Tools)\n",
    "# [Your custom functions remain the same]\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "def get_current_wait_time(hospital_name: str) -> str:\n",
    "    \"\"\"Simulate retrieving current wait time for a hospital.\"\"\"\n",
    "    if hospital_name not in hospitals_df['hospital_name'].values:\n",
    "        return f\"Hospital '{hospital_name}' does not exist.\"\n",
    "    time.sleep(1)  # Simulate API call delay\n",
    "    wait_time = random.randint(10, 120)  # Simulate wait time in minutes\n",
    "    return f\"The current wait time at {hospital_name} is approximately {wait_time} minutes.\"\n",
    "\n",
    "def get_physician_rating(physician_name: str) -> str:\n",
    "    \"\"\"Calculate average rating for a physician.\"\"\"\n",
    "    physician_reviews = reviews_df[reviews_df['physician_name'] == physician_name]\n",
    "    if physician_reviews.empty:\n",
    "        return f\"No reviews found for Dr. {physician_name}.\"\n",
    "    # Ensure 'rating' column exists; if not, create a simulated one\n",
    "    if 'rating' not in physician_reviews.columns:\n",
    "        physician_reviews['rating'] = [random.randint(1, 5) for _ in range(len(physician_reviews))]\n",
    "    average_rating = physician_reviews['rating'].mean()\n",
    "    return f\"The average rating for Dr. {physician_name} is {average_rating:.1f} out of 5.\"\n",
    "\n",
    "# Step 7: Wrap Functions as Tools\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Get Current Wait Time\",\n",
    "        func=get_current_wait_time,\n",
    "        description=\"Use this function to get the current wait time at a hospital. Input should be the hospital name.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Get Physician Rating\",\n",
    "        func=get_physician_rating,\n",
    "        description=\"Use this function to get the average rating of a physician based on patient reviews. Input should be the physician's name.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Step 8: Initialize the Agent\n",
    "# memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=False)\n",
    "# def preload_memory(memory, query):\n",
    "#     \"\"\"\n",
    "#     Preload the memory with the initial query to set context.\n",
    "#     \"\"\"\n",
    "#     memory.buffer.append({\"role\": \"user\", \"content\": query})\n",
    "\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=qa_chain,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    # memory=memory,\n",
    "    retriever=retriever,\n",
    "    max_iterations=3,\n",
    ")\n",
    "\n",
    "# Step 9: Test the Agent\n",
    "def handle_query(query):\n",
    "    try:\n",
    "        response = agent.run(query)\n",
    "    except Exception as e:\n",
    "        response = \"I'm sorry, I encountered an error while processing your request.\"\n",
    "        print(f\"Error: {str(e)}\")\n",
    "    return response\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Test Query 1\n",
    "    query = \"What are the salary and review for Dr. Smith?\"\n",
    "    answer = handle_query(query)\n",
    "    print(\"\\nQuery 1:\")\n",
    "    print(f\"Question: {query}\")\n",
    "    print(f\"Answer: {answer}\")\n",
    "    \n",
    "    # Test Query 2\n",
    "    query = \"What is the current wait time at Mercy Hospital?\"\n",
    "    print(\"\\nQuery 2:\")\n",
    "    print(f\"Question: {query}\")\n",
    "    print(f\"Answer: {answer}\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Device Name: NVIDIA GeForce RTX 3070\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device Name: {torch.cuda.get_device_name(0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas langchain transformers sentencepiece chromadb\n",
    "# !pip install -U langchain-community\n",
    "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "# !pip install sentence-transformers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
